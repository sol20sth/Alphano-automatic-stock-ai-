{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if os.environ.get('KERAS_BACKEND', 'tensorflow') == 'tensorflow':\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, \\\n",
    "        BatchNormalization, Dropout, MaxPooling1D, Flatten\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    from tensorflow.keras import backend\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "elif os.environ['KERAS_BACKEND'] == 'plaidml.keras.backend':\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input, Dense, LSTM, Conv1D, \\\n",
    "        BatchNormalization, Dropout, MaxPooling1D, Flatten\n",
    "    from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "class Network:\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    def __init__(self, input_dim=0, output_dim=0, lr=0.001, \n",
    "                shared_network=None, activation='sigmoid', loss='mse'):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.shared_network = shared_network\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.model = None\n",
    "\n",
    "    def predict(self, sample):\n",
    "        with self.lock:\n",
    "            pred = self.model.predict_on_batch(sample).flatten()\n",
    "            return pred\n",
    "\n",
    "    def train_on_batch(self, x, y):\n",
    "        loss = 0.\n",
    "        with self.lock:\n",
    "            history = self.model.fit(x, y, epochs=10, verbose=False)\n",
    "            loss += np.sum(history.history['loss'])\n",
    "        return loss\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        if model_path is not None and self.model is not None:\n",
    "            self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        if model_path is not None:\n",
    "            self.model.load_weights(model_path)\n",
    "\n",
    "    @classmethod\n",
    "    def get_shared_network(cls, net='dnn', num_steps=1, input_dim=0, output_dim=0):\n",
    "        # output_dim은 pytorch에서 필요\n",
    "        if net == 'dnn':\n",
    "            return DNN.get_network_head(Input((input_dim,)))\n",
    "        elif net == 'lstm':\n",
    "            return LSTMNetwork.get_network_head(Input((num_steps, input_dim)))\n",
    "        elif net == 'cnn':\n",
    "            return CNN.get_network_head(Input((num_steps, input_dim)))\n",
    "    \n",
    "\n",
    "class DNN(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        inp = None\n",
    "        output = None\n",
    "        if self.shared_network is None:\n",
    "            inp = Input((self.input_dim,))\n",
    "            output = self.get_network_head(inp).output\n",
    "        else:\n",
    "            inp = self.shared_network.input\n",
    "            output = self.shared_network.output\n",
    "        output = Dense(\n",
    "            self.output_dim, activation=self.activation, \n",
    "            kernel_initializer='random_normal')(output)\n",
    "        self.model = Model(inp, output)\n",
    "        self.model.compile(\n",
    "            optimizer=SGD(learning_rate=self.lr), loss=self.loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_network_head(inp):\n",
    "        output = Dense(256, activation='sigmoid', \n",
    "            kernel_initializer='random_normal')(inp)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = Dropout(0.1)(output)\n",
    "        output = Dense(128, activation='sigmoid', \n",
    "            kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = Dropout(0.1)(output)\n",
    "        output = Dense(64, activation='sigmoid', \n",
    "            kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = Dropout(0.1)(output)\n",
    "        output = Dense(32, activation='sigmoid', \n",
    "            kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = Dropout(0.1)(output)\n",
    "        return Model(inp, output)\n",
    "\n",
    "    def train_on_batch(self, x, y):\n",
    "        x = np.array(x).reshape((-1, self.input_dim))\n",
    "        return super().train_on_batch(x, y)\n",
    "\n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.input_dim))\n",
    "        return super().predict(sample)\n",
    "    \n",
    "\n",
    "class LSTMNetwork(Network):\n",
    "    def __init__(self, *args, num_steps=1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_steps = num_steps\n",
    "        inp = None\n",
    "        output = None\n",
    "        if self.shared_network is None:\n",
    "            inp = Input((self.num_steps, self.input_dim))\n",
    "            output = self.get_network_head(inp).output\n",
    "        else:\n",
    "            inp = self.shared_network.input\n",
    "            output = self.shared_network.output\n",
    "        output = Dense(\n",
    "            self.output_dim, activation=self.activation, \n",
    "            kernel_initializer='random_normal')(output)\n",
    "        self.model = Model(inp, output)\n",
    "        self.model.compile(\n",
    "            optimizer=SGD(learning_rate=self.lr), loss=self.loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_network_head(inp):\n",
    "        # cuDNN 사용을 위한 조건\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "        output = LSTM(256, dropout=0.1, return_sequences=True,\n",
    "                    kernel_initializer='random_normal')(inp)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = LSTM(128, dropout=0.1, return_sequences=True,\n",
    "                    kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = LSTM(64, dropout=0.1, return_sequences=True,\n",
    "                    kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = LSTM(32, dropout=0.1, kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        return Model(inp, output)\n",
    "\n",
    "    def train_on_batch(self, x, y):\n",
    "        x = np.array(x).reshape((-1, self.num_steps, self.input_dim))\n",
    "        return super().train_on_batch(x, y)\n",
    "\n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)\n",
    "\n",
    "\n",
    "class CNN(Network):\n",
    "    def __init__(self, *args, num_steps=1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_steps = num_steps\n",
    "        inp = None\n",
    "        output = None\n",
    "        if self.shared_network is None:\n",
    "            inp = Input((self.num_steps, self.input_dim, 1))\n",
    "            output = self.get_network_head(inp).output\n",
    "        else:\n",
    "            inp = self.shared_network.input\n",
    "            output = self.shared_network.output\n",
    "        output = Dense(\n",
    "            self.output_dim, activation=self.activation,\n",
    "            kernel_initializer='random_normal')(output)\n",
    "        self.model = Model(inp, output)\n",
    "        self.model.compile(\n",
    "            optimizer=SGD(learning_rate=self.lr), loss=self.loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_network_head(inp):\n",
    "        output = Conv1D(256, kernel_size=5,\n",
    "            padding='same', activation='sigmoid',\n",
    "            kernel_initializer='random_normal')(inp)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = MaxPooling1D(pool_size=2, padding='same')(output)\n",
    "        output = Dropout(0.1)(output)\n",
    "        output = Conv1D(64, kernel_size=5,\n",
    "            padding='same', activation='sigmoid',\n",
    "            kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = MaxPooling1D(pool_size=2, padding='same')(output)\n",
    "        output = Dropout(0.1)(output)\n",
    "        output = Conv1D(32, kernel_size=5,\n",
    "            padding='same', activation='sigmoid',\n",
    "            kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = MaxPooling1D(pool_size=2, padding='same')(output)\n",
    "        output = Dropout(0.1)(output)\n",
    "        output = Flatten()(output)\n",
    "        return Model(inp, output)\n",
    "\n",
    "    def train_on_batch(self, x, y):\n",
    "        x = np.array(x).reshape((-1, self.num_steps, self.input_dim, 1))\n",
    "        return super().train_on_batch(x, y)\n",
    "\n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape(\n",
    "            (-1, self.num_steps, self.input_dim, 1))\n",
    "        return super().predict(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
