{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.1.1-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "     --------------------------------------- 10.8/10.8 MB 11.7 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.22.4\n",
      "  Downloading numpy-1.26.1-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "     --------------------------------------- 15.8/15.8 MB 10.7 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ssafy\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ------------------------------------- 341.8/341.8 KB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ssafy\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.1 pandas-2.1.1 pytz-2023.3.post1 tzdata-2023.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.1-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "     ---------------------------------------- 9.3/9.3 MB 11.5 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\ssafy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.26.1)\n",
      "Collecting scipy>=1.5.0\n",
      "  Downloading scipy-1.11.3-cp39-cp39-win_amd64.whl (44.3 MB)\n",
      "     ---------------------------------------- 44.3/44.3 MB 9.6 MB/s eta 0:00:00\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.1 scipy-1.11.3 threadpoolctl-3.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "# CSV 파일들을 저장할 빈 DataFrame을 생성합니다.\n",
    "stock = pd.DataFrame()\n",
    "\n",
    "# data 폴더에 있는 모든 CSV 파일을 가져옵니다.\n",
    "csv_files = glob.glob('data/*.csv')\n",
    "\n",
    "# 모든 CSV 파일을 순회하며 데이터를 DataFrame에 추가합니다.\n",
    "for csv_file in csv_files:\n",
    "    tmp = pd.read_csv(csv_file)\n",
    "    stock = pd.concat([stock, tmp], ignore_index=True)\n",
    "\n",
    "# all_data DataFrame에 모든 CSV 파일의 내용이 포함됩니다.\n",
    "# Date,Open,High,Low,Close,Volume,Change,ma5,ma10,ma20,Kospi,S&P500,rsi,%K,%D,bb_upper,bb_sma,bb_lower,volume_ma5,momentum,high_low_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ma10', 'ma20'], ['ma10', 'S&P500'], ['ma10', 'Kospi'], ['ma10', 'rsi'], ['ma10', '%K'], ['ma10', '%D'], ['ma10', 'bb_upper'], ['ma10', 'bb_lower'], ['ma10', 'volume_ma5'], ['ma10', 'momentum'], ['ma10', 'high_low_diff'], ['ma20', 'S&P500'], ['ma20', 'Kospi'], ['ma20', 'rsi'], ['ma20', '%K'], ['ma20', '%D'], ['ma20', 'bb_upper'], ['ma20', 'bb_lower'], ['ma20', 'volume_ma5'], ['ma20', 'momentum'], ['ma20', 'high_low_diff'], ['S&P500', 'Kospi'], ['S&P500', 'rsi'], ['S&P500', '%K'], ['S&P500', '%D'], ['S&P500', 'bb_upper'], ['S&P500', 'bb_lower'], ['S&P500', 'volume_ma5'], ['S&P500', 'momentum'], ['S&P500', 'high_low_diff'], ['Kospi', 'rsi'], ['Kospi', '%K'], ['Kospi', '%D'], ['Kospi', 'bb_upper'], ['Kospi', 'bb_lower'], ['Kospi', 'volume_ma5'], ['Kospi', 'momentum'], ['Kospi', 'high_low_diff'], ['rsi', '%K'], ['rsi', '%D'], ['rsi', 'bb_upper'], ['rsi', 'bb_lower'], ['rsi', 'volume_ma5'], ['rsi', 'momentum'], ['rsi', 'high_low_diff'], ['%K', '%D'], ['%K', 'bb_upper'], ['%K', 'bb_lower'], ['%K', 'volume_ma5'], ['%K', 'momentum'], ['%K', 'high_low_diff'], ['%D', 'bb_upper'], ['%D', 'bb_lower'], ['%D', 'volume_ma5'], ['%D', 'momentum'], ['%D', 'high_low_diff'], ['bb_upper', 'bb_lower'], ['bb_upper', 'volume_ma5'], ['bb_upper', 'momentum'], ['bb_upper', 'high_low_diff'], ['bb_lower', 'volume_ma5'], ['bb_lower', 'momentum'], ['bb_lower', 'high_low_diff'], ['volume_ma5', 'momentum'], ['volume_ma5', 'high_low_diff'], ['momentum', 'high_low_diff']]\n",
      "Epoch 1/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 1: val_loss improved from inf to 0.00158, saving model to ./checkpoint/LSTM\\LSTM_ma10_ma20.h5\n",
      "992/992 [==============================] - 22s 17ms/step - loss: 0.0071 - val_loss: 0.0016\n",
      "Epoch 2/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 2: val_loss improved from 0.00158 to 0.00150, saving model to ./checkpoint/LSTM\\LSTM_ma10_ma20.h5\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 3: val_loss improved from 0.00150 to 0.00140, saving model to ./checkpoint/LSTM\\LSTM_ma10_ma20.h5\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 4/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 4: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 5/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 5: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 6/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 6: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0016\n",
      "Epoch 7/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 7: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 8/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 8: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0016\n",
      "Epoch 9/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 9: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0016\n",
      "Epoch 10/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 10: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 11/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 11: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 12/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 12: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 13/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 13: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 14/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 14: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 15/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 15: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 16/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 16: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 17/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 17: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 18/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 18: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 19/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 19: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 16s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 20/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 20: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 21/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 21: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 22/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 22: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 23/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 23: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "[[         Open    High     Low   Close  Volume      ma10       ma20\n",
      "49564  879583  904291  875630  880572   96530  859225.0  846475.95\n",
      "49565  875630  885513  854876  881560   60446  864265.3  847810.15\n",
      "49566  865747  893420  861794  889467   59299  865056.0  849589.10\n",
      "49567  890455  898361  874642  890455   47530  867526.7  853443.45\n",
      "49568  875630  881560  839063  866736   94814  869700.9  855963.60\n",
      "49569  872665  909232  871677  903303  140974  876124.8  860608.60\n",
      "49570  903302  908244  889466  905280   97750  882944.1  865401.85\n",
      "49571  896384  905279  890455  890455   39993  886007.8  868762.05\n",
      "49572  896384  899349  860806  861795   79128  884821.9  869157.40\n",
      "49573  875630  884525  861794  879584   75796  884920.7  870046.85\n",
      "49574  888478  901326  884525  893420   80391  886205.5  872715.25\n",
      "49575  906267  947776  906267  946788  288779  892728.3  878496.80\n",
      "49576  941846  955682  931963  943823  141827  898163.9  881609.95\n",
      "49577  949752  950741  919115  924057   65625  901524.1  884525.40\n",
      "49578  925045  933940  913185  918127   48913  906663.2  888182.05\n",
      "49579  904291  914174  887490  890455   81385  905378.4  890751.60\n",
      "49580  890455  894408  871677  885514   69075  903401.8  893172.95\n",
      "49581  874642  886501  874642  875631   57862  903204.1  894013.00\n",
      "49582  880572  882548  872665  880572   91909  903302.9  894111.80\n",
      "49583  888478  894408  873654  879584   72407  901919.3  894062.40]]\n",
      "[[         Open    High     Low   Close  Volume      ma10      ma20\n",
      "49584  947776  947776  885513  892432  175056  896483.7  894606.0]]\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[873273.44, 889380.25, 846137.9, 871276.9, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/ma20']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', 'ma20']\n",
      "Epoch 1/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0088\n",
      "Epoch 1: val_loss improved from inf to 0.00430, saving model to ./checkpoint/LSTM\\LSTM_ma10_S&P500.h5\n",
      "992/992 [==============================] - 32s 22ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 2/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 2: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0086 - val_loss: 0.0046\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0086\n",
      "Epoch 3: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0086 - val_loss: 0.0044\n",
      "Epoch 4/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 4: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0086 - val_loss: 0.0046\n",
      "Epoch 5/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 5: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0086 - val_loss: 0.0046\n",
      "Epoch 6/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 6: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 7/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 7: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 8/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 8: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 9/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 9: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 10/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 10: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 11/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 11: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0086 - val_loss: 0.0046\n",
      "Epoch 12/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 12: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 13/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 13: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0086 - val_loss: 0.0046\n",
      "Epoch 14/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 14: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 15/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0086\n",
      "Epoch 15: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0086 - val_loss: 0.0046\n",
      "Epoch 16/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0086\n",
      "Epoch 16: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 17/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 17: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 18/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 18: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0086 - val_loss: 0.0046\n",
      "Epoch 19/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 19: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0086 - val_loss: 0.0046\n",
      "Epoch 20/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 20: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0086 - val_loss: 0.0045\n",
      "Epoch 21/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0086\n",
      "Epoch 21: val_loss did not improve from 0.00430\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0086 - val_loss: 0.0046\n",
      "[[         Open    High     Low   Close  Volume      ma10       S&P500\n",
      "49564  879583  904291  875630  880572   96530  859225.0  4513.040039\n",
      "49565  875630  885513  854876  881560   60446  864265.3  4577.100098\n",
      "49566  865747  893420  861794  889467   59299  865056.0  4538.430176\n",
      "49567  890455  898361  874642  890455   47530  867526.7  4591.669922\n",
      "49568  875630  881560  839063  866736   94814  869700.9  4686.750000\n",
      "49569  872665  909232  871677  903303  140974  876124.8  4701.209961\n",
      "49570  903302  908244  889466  905280   97750  882944.1  4667.450195\n",
      "49571  896384  905279  890455  890455   39993  886007.8  4712.020020\n",
      "49572  896384  899349  860806  861795   79128  884821.9  4668.970215\n",
      "49573  875630  884525  861794  879584   75796  884920.7  4634.089844\n",
      "49574  888478  901326  884525  893420   80391  886205.5  4709.850098\n",
      "49575  906267  947776  906267  946788  288779  892728.3  4668.669922\n",
      "49576  941846  955682  931963  943823  141827  898163.9  4620.640137\n",
      "49577  949752  950741  919115  924057   65625  901524.1  4568.020020\n",
      "49578  925045  933940  913185  918127   48913  906663.2  4649.229980\n",
      "49579  904291  914174  887490  890455   81385  905378.4  4696.560059\n",
      "49580  890455  894408  871677  885514   69075  903401.8  4725.790039\n",
      "49581  874642  886501  874642  875631   57862  903204.1  4791.189941\n",
      "49582  880572  882548  872665  880572   91909  903302.9  4786.350098\n",
      "49583  888478  894408  873654  879584   72407  901919.3  4793.060059]]\n",
      "[[         Open    High     Low   Close  Volume      ma10      S&P500\n",
      "49584  947776  947776  885513  892432  175056  896483.7  4778.72998]]\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[872811.56, 889406.75, 846680.4, 870086.44, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/S&P500']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', 'S&P500']\n",
      "Epoch 1/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0098\n",
      "Epoch 1: val_loss improved from inf to 0.00441, saving model to ./checkpoint/LSTM\\LSTM_ma10_Kospi.h5\n",
      "992/992 [==============================] - 27s 19ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 2/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0095\n",
      "Epoch 2: val_loss did not improve from 0.00441\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0046\n",
      "Epoch 3/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 3: val_loss improved from 0.00441 to 0.00440, saving model to ./checkpoint/LSTM\\LSTM_ma10_Kospi.h5\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 4/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 4: val_loss improved from 0.00440 to 0.00438, saving model to ./checkpoint/LSTM\\LSTM_ma10_Kospi.h5\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 5/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 5: val_loss improved from 0.00438 to 0.00437, saving model to ./checkpoint/LSTM\\LSTM_ma10_Kospi.h5\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 6/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0095\n",
      "Epoch 6: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0046\n",
      "Epoch 7/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 7: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 8/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 8: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0045\n",
      "Epoch 9/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 9: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 10/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 10: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 11/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 11: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 12/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 12: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0095 - val_loss: 0.0046\n",
      "Epoch 13/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0095\n",
      "Epoch 13: val_loss improved from 0.00437 to 0.00437, saving model to ./checkpoint/LSTM\\LSTM_ma10_Kospi.h5\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 14/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 14: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 15/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 15: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 16/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 16: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 17/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0095\n",
      "Epoch 17: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0095 - val_loss: 0.0045\n",
      "Epoch 18/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 18: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 19/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 19: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0045\n",
      "Epoch 20/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 20: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0045\n",
      "Epoch 21/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 21: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 22/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 22: val_loss did not improve from 0.00437\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 23/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 23: val_loss improved from 0.00437 to 0.00435, saving model to ./checkpoint/LSTM\\LSTM_ma10_Kospi.h5\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 24/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 24: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0045\n",
      "Epoch 25/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 25: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 26/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 26: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 27/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 27: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 28/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 28: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 29/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 29: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0095 - val_loss: 0.0045\n",
      "Epoch 30/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0095\n",
      "Epoch 30: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 31/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0095\n",
      "Epoch 31: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 32/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 32: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 33/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 33: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 34/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 34: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0045\n",
      "Epoch 35/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 35: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0095 - val_loss: 0.0045\n",
      "Epoch 36/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 36: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 37/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 37: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 38/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 38: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 39/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 39: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 40/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 40: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 41/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 41: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 42/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 42: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0095 - val_loss: 0.0045\n",
      "Epoch 43/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0095\n",
      "Epoch 43: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "[[         Open    High     Low   Close  Volume      ma10        Kospi\n",
      "49564  879583  904291  875630  880572   96530  859225.0  2899.719971\n",
      "49565  875630  885513  854876  881560   60446  864265.3  2945.270020\n",
      "49566  865747  893420  861794  889467   59299  865056.0  2968.330078\n",
      "49567  890455  898361  874642  890455   47530  867526.7  2973.250000\n",
      "49568  875630  881560  839063  866736   94814  869700.9  2991.719971\n",
      "49569  872665  909232  871677  903303  140974  876124.8  3001.800049\n",
      "49570  903302  908244  889466  905280   97750  882944.1  3029.570068\n",
      "49571  896384  905279  890455  890455   39993  886007.8  3010.229980\n",
      "49572  896384  899349  860806  861795   79128  884821.9  3001.659912\n",
      "49573  875630  884525  861794  879584   75796  884920.7  2987.949951\n",
      "49574  888478  901326  884525  893420   80391  886205.5  2989.389893\n",
      "49575  906267  947776  906267  946788  288779  892728.3  3006.409912\n",
      "49576  941846  955682  931963  943823  141827  898163.9  3017.729980\n",
      "49577  949752  950741  919115  924057   65625  901524.1  2963.000000\n",
      "49578  925045  933940  913185  918127   48913  906663.2  2975.030029\n",
      "49579  904291  914174  887490  890455   81385  905378.4  2984.479980\n",
      "49580  890455  894408  871677  885514   69075  903401.8  2998.169922\n",
      "49581  874642  886501  874642  875631   57862  903204.1  2999.550049\n",
      "49582  880572  882548  872665  880572   91909  903302.9  3020.239990\n",
      "49583  888478  894408  873654  879584   72407  901919.3  2993.290039]]\n",
      "[[         Open    High     Low   Close  Volume      ma10        Kospi\n",
      "49584  947776  947776  885513  892432  175056  896483.7  2977.649902]]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[873322.8, 888084.5, 848032.06, 869727.6, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/Kospi']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', 'Kospi']\n",
      "Epoch 1/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0080\n",
      "Epoch 1: val_loss improved from inf to 0.00332, saving model to ./checkpoint/LSTM\\LSTM_ma10_rsi.h5\n",
      "992/992 [==============================] - 28s 20ms/step - loss: 0.0080 - val_loss: 0.0033\n",
      "Epoch 2/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 2: val_loss improved from 0.00332 to 0.00295, saving model to ./checkpoint/LSTM\\LSTM_ma10_rsi.h5\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0078 - val_loss: 0.0029\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 3: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0032\n",
      "Epoch 4/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 4: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 5/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 5: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 6/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 6: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0029\n",
      "Epoch 7/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 7: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 8/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 8: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0031\n",
      "Epoch 9/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 9: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 10/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 10: val_loss improved from 0.00295 to 0.00293, saving model to ./checkpoint/LSTM\\LSTM_ma10_rsi.h5\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0029\n",
      "Epoch 11/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 11: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 12/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 12: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0078 - val_loss: 0.0029\n",
      "Epoch 13/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 13: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 14/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0077\n",
      "Epoch 14: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 15/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0077\n",
      "Epoch 15: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 16/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 16: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0029\n",
      "Epoch 17/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 17: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 18/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 18: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0078 - val_loss: 0.0031\n",
      "Epoch 19/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 19: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 20/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 20: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 21/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 21: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0031\n",
      "Epoch 22/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 22: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 23/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 23: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0031\n",
      "Epoch 24/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0077\n",
      "Epoch 24: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 25/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 25: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0031\n",
      "Epoch 26/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 26: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 27/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 27: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 28/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 28: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0031\n",
      "Epoch 29/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 29: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0029\n",
      "Epoch 30/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 30: val_loss did not improve from 0.00293\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "[[         Open    High     Low   Close  Volume      ma10        rsi\n",
      "49564  879583  904291  875630  880572   96530  859225.0  66.063375\n",
      "49565  875630  885513  854876  881560   60446  864265.3  66.515729\n",
      "49566  865747  893420  861794  889467   59299  865056.0  65.581408\n",
      "49567  890455  898361  874642  890455   47530  867526.7  60.000000\n",
      "49568  875630  881560  839063  866736   94814  869700.9  51.243663\n",
      "49569  872665  909232  871677  903303  140974  876124.8  64.814893\n",
      "49570  903302  908244  889466  905280   97750  882944.1  67.942852\n",
      "49571  896384  905279  890455  890455   39993  886007.8  52.601257\n",
      "49572  896384  899349  860806  861795   79128  884821.9  48.924781\n",
      "49573  875630  884525  861794  879584   75796  884920.7  59.562751\n",
      "49574  888478  901326  884525  893420   80391  886205.5  64.397872\n",
      "49575  906267  947776  906267  946788  288779  892728.3  69.642873\n",
      "49576  941846  955682  931963  943823  141827  898163.9  66.666667\n",
      "49577  949752  950741  919115  924057   65625  901524.1  60.087604\n",
      "49578  925045  933940  913185  918127   48913  906663.2  58.189589\n",
      "49579  904291  914174  887490  890455   81385  905378.4  51.737515\n",
      "49580  890455  894408  871677  885514   69075  903401.8  49.218784\n",
      "49581  874642  886501  874642  875631   57862  903204.1  47.169891\n",
      "49582  880572  882548  872665  880572   91909  903302.9  52.845508\n",
      "49583  888478  894408  873654  879584   72407  901919.3  44.285707]]\n",
      "[[         Open    High     Low   Close  Volume      ma10       rsi\n",
      "49584  947776  947776  885513  892432  175056  896483.7  47.05877]]\n",
      "1/1 [==============================] - 1s 880ms/step\n",
      "[872158.44, 887168.25, 847040.25, 869242.75, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/rsi']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', 'rsi']\n",
      "Epoch 1/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 1: val_loss improved from inf to 0.00602, saving model to ./checkpoint/LSTM\\LSTM_ma10_%K.h5\n",
      "992/992 [==============================] - 29s 19ms/step - loss: 0.0111 - val_loss: 0.0060\n",
      "Epoch 2/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 2: val_loss improved from 0.00602 to 0.00589, saving model to ./checkpoint/LSTM\\LSTM_ma10_%K.h5\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0108 - val_loss: 0.0059\n",
      "Epoch 3/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 3: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 4/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 4: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 5/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 5: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0108 - val_loss: 0.0059\n",
      "Epoch 6/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 6: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 7/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 7: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0108 - val_loss: 0.0059\n",
      "Epoch 8/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 8: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 9/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 9: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 10/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 10: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0108 - val_loss: 0.0059\n",
      "Epoch 11/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 11: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0108 - val_loss: 0.0059\n",
      "Epoch 12/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 12: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 13/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 13: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0108 - val_loss: 0.0059\n",
      "Epoch 14/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 14: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0108 - val_loss: 0.0059\n",
      "Epoch 15/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 15: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 16/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 16: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 18s 19ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 17/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 17: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 18/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 18: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0108 - val_loss: 0.0059\n",
      "Epoch 19/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 19: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 20/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0108\n",
      "Epoch 20: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 18s 19ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 21/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 21: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 22/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 22: val_loss did not improve from 0.00589\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "[[         Open    High     Low   Close  Volume      ma10         %K\n",
      "49564  879583  904291  875630  880572   96530  859225.0  73.394972\n",
      "49565  875630  885513  854876  881560   60446  864265.3  72.000121\n",
      "49566  865747  893420  861794  889467   59299  865056.0  76.745144\n",
      "49567  890455  898361  874642  890455   47530  867526.7  77.907592\n",
      "49568  875630  881560  839063  866736   94814  869700.9  50.000588\n",
      "49569  872665  909232  871677  903303  140974  876124.8  93.024131\n",
      "49570  903302  908244  889466  905280   97750  882944.1  94.873325\n",
      "49571  896384  905279  890455  890455   39993  886007.8  75.641807\n",
      "49572  896384  899349  860806  861795   79128  884821.9  38.463035\n",
      "49573  875630  884525  861794  879584   75796  884920.7  61.539559\n",
      "49574  888478  901326  884525  893420   80391  886205.5  79.488111\n",
      "49575  906267  947776  906267  946788  288779  892728.3  99.091185\n",
      "49576  941846  955682  931963  943823  141827  898163.9  89.830988\n",
      "49577  949752  950741  919115  924057   65625  901524.1  72.881777\n",
      "49578  925045  933940  913185  918127   48913  906663.2  67.796843\n",
      "49579  904291  914174  887490  890455   81385  905378.4  44.068291\n",
      "49580  890455  894408  871677  885514   69075  903401.8  39.831417\n",
      "49581  874642  886501  874642  875631   57862  903204.1  31.356811\n",
      "49582  880572  882548  872665  880572   91909  903302.9  20.833509\n",
      "49583  888478  894408  873654  879584   72407  901919.3  19.792150]]\n",
      "[[         Open    High     Low   Close  Volume      ma10         %K\n",
      "49584  947776  947776  885513  892432  175056  896483.7  33.334036]]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[873042.94, 888301.9, 850078.4, 875142.1, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/%K']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', '%K']\n",
      "Epoch 1/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0113\n",
      "Epoch 1: val_loss improved from inf to 0.00609, saving model to ./checkpoint/LSTM\\LSTM_ma10_%D.h5\n",
      "992/992 [==============================] - 23s 18ms/step - loss: 0.0113 - val_loss: 0.0061\n",
      "Epoch 2/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 2: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 3/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 3: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 4/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 4: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 5/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0111\n",
      "Epoch 5: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 6/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0111\n",
      "Epoch 6: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 7/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0111\n",
      "Epoch 7: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 8/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 8: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 9/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0111\n",
      "Epoch 9: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 10/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0111\n",
      "Epoch 10: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 11/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 11: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 12/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 12: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 13/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 13: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 14/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0111\n",
      "Epoch 14: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 15/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 15: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 16/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 16: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0111 - val_loss: 0.0061\n",
      "Epoch 17/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 17: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 18/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 18: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 19ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 19/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 19: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0063\n",
      "Epoch 20/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 20: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 21/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 21: val_loss did not improve from 0.00609\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0111 - val_loss: 0.0062\n",
      "[[         Open    High     Low   Close  Volume      ma10         %D\n",
      "49564  879583  904291  875630  880572   96530  859225.0  70.642877\n",
      "49565  875630  885513  854876  881560   60446  864265.3  72.318583\n",
      "49566  865747  893420  861794  889467   59299  865056.0  74.046746\n",
      "49567  890455  898361  874642  890455   47530  867526.7  75.550953\n",
      "49568  875630  881560  839063  866736   94814  869700.9  68.217775\n",
      "49569  872665  909232  871677  903303  140974  876124.8  73.644104\n",
      "49570  903302  908244  889466  905280   97750  882944.1  79.299348\n",
      "49571  896384  905279  890455  890455   39993  886007.8  87.846421\n",
      "49572  896384  899349  860806  861795   79128  884821.9  69.659389\n",
      "49573  875630  884525  861794  879584   75796  884920.7  58.548134\n",
      "49574  888478  901326  884525  893420   80391  886205.5  59.830235\n",
      "49575  906267  947776  906267  946788  288779  892728.3  80.039618\n",
      "49576  941846  955682  931963  943823  141827  898163.9  89.470095\n",
      "49577  949752  950741  919115  924057   65625  901524.1  87.267984\n",
      "49578  925045  933940  913185  918127   48913  906663.2  76.836536\n",
      "49579  904291  914174  887490  890455   81385  905378.4  61.582304\n",
      "49580  890455  894408  871677  885514   69075  903401.8  50.565517\n",
      "49581  874642  886501  874642  875631   57862  903204.1  38.418840\n",
      "49582  880572  882548  872665  880572   91909  903302.9  30.673912\n",
      "49583  888478  894408  873654  879584   72407  901919.3  23.994157]]\n",
      "[[         Open    High     Low   Close  Volume      ma10         %D\n",
      "49584  947776  947776  885513  892432  175056  896483.7  24.653232]]\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[877383.6, 885248.9, 845895.5, 871051.0, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/%D']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', '%D']\n",
      "Epoch 1/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0070\n",
      "Epoch 1: val_loss improved from inf to 0.00154, saving model to ./checkpoint/LSTM\\LSTM_ma10_bb_upper.h5\n",
      "992/992 [==============================] - 26s 18ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 2/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 2: val_loss improved from 0.00154 to 0.00143, saving model to ./checkpoint/LSTM\\LSTM_ma10_bb_upper.h5\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 3/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 3: val_loss did not improve from 0.00143\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 4/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 4: val_loss did not improve from 0.00143\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 5/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 5: val_loss improved from 0.00143 to 0.00141, saving model to ./checkpoint/LSTM\\LSTM_ma10_bb_upper.h5\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 6/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 6: val_loss improved from 0.00141 to 0.00140, saving model to ./checkpoint/LSTM\\LSTM_ma10_bb_upper.h5\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 7/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 7: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0016\n",
      "Epoch 8/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 8: val_loss improved from 0.00140 to 0.00140, saving model to ./checkpoint/LSTM\\LSTM_ma10_bb_upper.h5\n",
      "992/992 [==============================] - 18s 19ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 9/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0070\n",
      "Epoch 9: val_loss improved from 0.00140 to 0.00140, saving model to ./checkpoint/LSTM\\LSTM_ma10_bb_upper.h5\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 10/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 10: val_loss improved from 0.00140 to 0.00140, saving model to ./checkpoint/LSTM\\LSTM_ma10_bb_upper.h5\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 11/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0070\n",
      "Epoch 11: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 12/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0070\n",
      "Epoch 12: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 13/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 13: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 14/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 14: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 15/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 15: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 16/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 16: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 17/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 17: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 18/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0070\n",
      "Epoch 18: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 19/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 19: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 20/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 20: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 21/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 21: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 22/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 22: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 23/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 23: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 24/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 24: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 25/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 25: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 26/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0070\n",
      "Epoch 26: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 27/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 27: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 28/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 28: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 29/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 29: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0070 - val_loss: 0.0015\n",
      "Epoch 30/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Epoch 30: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0070 - val_loss: 0.0014\n",
      "[[         Open    High     Low   Close  Volume      ma10       bb_upper\n",
      "49564  879583  904291  875630  880572   96530  859225.0  895809.711065\n",
      "49565  875630  885513  854876  881560   60446  864265.3  898567.385325\n",
      "49566  865747  893420  861794  889467   59299  865056.0  903495.716310\n",
      "49567  890455  898361  874642  890455   47530  867526.7  908264.144220\n",
      "49568  875630  881560  839063  866736   94814  869700.9  908147.109693\n",
      "49569  872665  909232  871677  903303  140974  876124.8  913422.548328\n",
      "49570  903302  908244  889466  905280   97750  882944.1  916744.202028\n",
      "49571  896384  905279  890455  890455   39993  886007.8  915296.773194\n",
      "49572  896384  899349  860806  861795   79128  884821.9  912425.017843\n",
      "49573  875630  884525  861794  879584   75796  884920.7  913209.106806\n",
      "49574  888478  901326  884525  893420   80391  886205.5  915660.219769\n",
      "49575  906267  947776  906267  946788  288779  892728.3  929537.195099\n",
      "49576  941846  955682  931963  943823  141827  898163.9  938078.852220\n",
      "49577  949752  950741  919115  924057   65625  901524.1  943081.683350\n",
      "49578  925045  933940  913185  918127   48913  906663.2  946595.053090\n",
      "49579  904291  914174  887490  890455   81385  905378.4  945119.576307\n",
      "49580  890455  894408  871677  885514   69075  903401.8  941792.129674\n",
      "49581  874642  886501  874642  875631   57862  903204.1  940779.773037\n",
      "49582  880572  882548  872665  880572   91909  903302.9  940593.051556\n",
      "49583  888478  894408  873654  879584   72407  901919.3  940573.892778]]\n",
      "[[         Open    High     Low   Close  Volume      ma10       bb_upper\n",
      "49584  947776  947776  885513  892432  175056  896483.7  940724.272154]]\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[873352.25, 888011.56, 847742.44, 869082.9, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/bb_upper']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', 'bb_upper']\n",
      "Epoch 1/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 1: val_loss improved from inf to 0.00139, saving model to ./checkpoint/LSTM\\LSTM_ma10_bb_lower.h5\n",
      "992/992 [==============================] - 28s 20ms/step - loss: 0.0072 - val_loss: 0.0014\n",
      "Epoch 2/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 2: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0016\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 3: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0016\n",
      "Epoch 4/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 4: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 5/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 5: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 6/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 6: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 19ms/step - loss: 0.0071 - val_loss: 0.0016\n",
      "Epoch 7/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 7: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 8/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 8: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 9/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 9: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 10/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 10: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 11/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 11: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 12/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 12: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 13/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 13: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 14/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 14: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 20s 20ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 15/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 15: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 16/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 16: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 17/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 17: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 18/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 18: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 19/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 19: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 20/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 20: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 21/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 21: val_loss did not improve from 0.00139\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "[[         Open    High     Low   Close  Volume      ma10       bb_lower\n",
      "49564  879583  904291  875630  880572   96530  859225.0  799810.588935\n",
      "49565  875630  885513  854876  881560   60446  864265.3  798831.814675\n",
      "49566  865747  893420  861794  889467   59299  865056.0  797362.583690\n",
      "49567  890455  898361  874642  890455   47530  867526.7  796250.855780\n",
      "49568  875630  881560  839063  866736   94814  869700.9  801704.690307\n",
      "49569  872665  909232  871677  903303  140974  876124.8  805126.251672\n",
      "49570  903302  908244  889466  905280   97750  882944.1  811292.297972\n",
      "49571  896384  905279  890455  890455   39993  886007.8  820843.726806\n",
      "49572  896384  899349  860806  861795   79128  884821.9  827569.882157\n",
      "49573  875630  884525  861794  879584   75796  884920.7  829355.393194\n",
      "49574  888478  901326  884525  893420   80391  886205.5  830066.780231\n",
      "49575  906267  947776  906267  946788  288779  892728.3  826863.404901\n",
      "49576  941846  955682  931963  943823  141827  898163.9  829588.347780\n",
      "49577  949752  950741  919115  924057   65625  901524.1  828835.216650\n",
      "49578  925045  933940  913185  918127   48913  906663.2  830559.746910\n",
      "49579  904291  914174  887490  890455   81385  905378.4  836581.323693\n",
      "49580  890455  894408  871677  885514   69075  903401.8  844553.770326\n",
      "49581  874642  886501  874642  875631   57862  903204.1  847147.426963\n",
      "49582  880572  882548  872665  880572   91909  903302.9  848025.948444\n",
      "49583  888478  894408  873654  879584   72407  901919.3  848143.907222]]\n",
      "[[         Open    High     Low   Close  Volume      ma10       bb_lower\n",
      "49584  947776  947776  885513  892432  175056  896483.7  849179.527846]]\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[871706.75, 890590.5, 845051.0, 871553.7, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/bb_lower']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', 'bb_lower']\n",
      "Epoch 1/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 1: val_loss improved from inf to 0.00117, saving model to ./checkpoint/LSTM\\LSTM_ma10_volume_ma5.h5\n",
      "992/992 [==============================] - 29s 20ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 2/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 2: val_loss did not improve from 0.00117\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 3/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 3: val_loss did not improve from 0.00117\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 4/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 4: val_loss did not improve from 0.00117\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 5/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0059\n",
      "Epoch 5: val_loss did not improve from 0.00117\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 6/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 6: val_loss did not improve from 0.00117\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0059 - val_loss: 0.0013\n",
      "Epoch 7/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 7: val_loss improved from 0.00117 to 0.00116, saving model to ./checkpoint/LSTM\\LSTM_ma10_volume_ma5.h5\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 8/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 8: val_loss did not improve from 0.00116\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 9/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 9: val_loss improved from 0.00116 to 0.00115, saving model to ./checkpoint/LSTM\\LSTM_ma10_volume_ma5.h5\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 10/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 10: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 11/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 11: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 12/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 12: val_loss improved from 0.00115 to 0.00114, saving model to ./checkpoint/LSTM\\LSTM_ma10_volume_ma5.h5\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0011\n",
      "Epoch 13/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 13: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 14/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 14: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 15/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 15: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 16/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0059\n",
      "Epoch 16: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 17/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 17: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 18/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 18: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 19/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 19: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 19s 20ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 20/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 20: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 15s 16ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 21/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0059\n",
      "Epoch 21: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 22/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 22: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 19s 20ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 23/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0059\n",
      "Epoch 23: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 24/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 24: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 25/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 25: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 26/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 26: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 27/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 27: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 28/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 28: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 18s 19ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 29/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 29: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 30/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 30: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 31/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 31: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 32/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 32: val_loss did not improve from 0.00114\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "[[         Open    High     Low   Close  Volume      ma10  volume_ma5\n",
      "49564  879583  904291  875630  880572   96530  859225.0     89361.4\n",
      "49565  875630  885513  854876  881560   60446  864265.3     92991.8\n",
      "49566  865747  893420  861794  889467   59299  865056.0     90411.2\n",
      "49567  890455  898361  874642  890455   47530  867526.7     76922.4\n",
      "49568  875630  881560  839063  866736   94814  869700.9     71723.8\n",
      "49569  872665  909232  871677  903303  140974  876124.8     80612.6\n",
      "49570  903302  908244  889466  905280   97750  882944.1     88073.4\n",
      "49571  896384  905279  890455  890455   39993  886007.8     84212.2\n",
      "49572  896384  899349  860806  861795   79128  884821.9     90531.8\n",
      "49573  875630  884525  861794  879584   75796  884920.7     86728.2\n",
      "49574  888478  901326  884525  893420   80391  886205.5     74611.6\n",
      "49575  906267  947776  906267  946788  288779  892728.3    112817.4\n",
      "49576  941846  955682  931963  943823  141827  898163.9    133184.2\n",
      "49577  949752  950741  919115  924057   65625  901524.1    130483.6\n",
      "49578  925045  933940  913185  918127   48913  906663.2    125107.0\n",
      "49579  904291  914174  887490  890455   81385  905378.4    125305.8\n",
      "49580  890455  894408  871677  885514   69075  903401.8     81365.0\n",
      "49581  874642  886501  874642  875631   57862  903204.1     64572.0\n",
      "49582  880572  882548  872665  880572   91909  903302.9     69828.8\n",
      "49583  888478  894408  873654  879584   72407  901919.3     74527.6]]\n",
      "[[         Open    High     Low   Close  Volume      ma10  volume_ma5\n",
      "49584  947776  947776  885513  892432  175056  896483.7     93261.8]]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[873525.4, 887988.8, 847044.8, 868574.0, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/volume_ma5']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', 'volume_ma5']\n",
      "Epoch 1/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0059\n",
      "Epoch 1: val_loss improved from inf to 0.00118, saving model to ./checkpoint/LSTM\\LSTM_ma10_momentum.h5\n",
      "992/992 [==============================] - 30s 22ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 2/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 2: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 3: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 4/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 4: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 5/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 5: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0013\n",
      "Epoch 6/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 6: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 7/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 7: val_loss improved from 0.00118 to 0.00117, saving model to ./checkpoint/LSTM\\LSTM_ma10_momentum.h5\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 8/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 8: val_loss did not improve from 0.00117\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0014\n",
      "Epoch 9/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 9: val_loss improved from 0.00117 to 0.00116, saving model to ./checkpoint/LSTM\\LSTM_ma10_momentum.h5\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 10/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 10: val_loss did not improve from 0.00116\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 11/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 11: val_loss did not improve from 0.00116\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 12/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 12: val_loss improved from 0.00116 to 0.00116, saving model to ./checkpoint/LSTM\\LSTM_ma10_momentum.h5\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 13/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 13: val_loss did not improve from 0.00116\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0013\n",
      "Epoch 14/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 14: val_loss improved from 0.00116 to 0.00115, saving model to ./checkpoint/LSTM\\LSTM_ma10_momentum.h5\n",
      "992/992 [==============================] - 15s 16ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 15/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 15: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 16/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 16: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 17/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 17: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 18/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 18: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 19/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 19: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 20/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 20: val_loss improved from 0.00115 to 0.00115, saving model to ./checkpoint/LSTM\\LSTM_ma10_momentum.h5\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 21/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 21: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 22/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 22: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 23/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 23: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 24/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 24: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 25/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 25: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 26/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 26: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 27/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 27: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0013\n",
      "Epoch 28/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 28: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 29/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 29: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 30/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 30: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 31/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 31: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 17s 18ms/step - loss: 0.0058 - val_loss: 0.0013\n",
      "Epoch 32/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 32: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 33/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 33: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 34/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 34: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 35/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 35: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 36/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 36: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0058 - val_loss: 0.0013\n",
      "Epoch 37/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 37: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 38/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 38: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 39/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 39: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0058 - val_loss: 0.0013\n",
      "Epoch 40/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 40: val_loss did not improve from 0.00115\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "[[         Open    High     Low   Close  Volume      ma10  momentum\n",
      "49564  879583  904291  875630  880572   96530  859225.0   35578.0\n",
      "49565  875630  885513  854876  881560   60446  864265.3   42496.0\n",
      "49566  865747  893420  861794  889467   59299  865056.0   29649.0\n",
      "49567  890455  898361  874642  890455   47530  867526.7   16801.0\n",
      "49568  875630  881560  839063  866736   94814  869700.9  -11860.0\n",
      "49569  872665  909232  871677  903303  140974  876124.8   22731.0\n",
      "49570  903302  908244  889466  905280   97750  882944.1   23720.0\n",
      "49571  896384  905279  890455  890455   39993  886007.8     988.0\n",
      "49572  896384  899349  860806  861795   79128  884821.9  -28660.0\n",
      "49573  875630  884525  861794  879584   75796  884920.7   12848.0\n",
      "49574  888478  901326  884525  893420   80391  886205.5   -9883.0\n",
      "49575  906267  947776  906267  946788  288779  892728.3   41508.0\n",
      "49576  941846  955682  931963  943823  141827  898163.9   53368.0\n",
      "49577  949752  950741  919115  924057   65625  901524.1   62262.0\n",
      "49578  925045  933940  913185  918127   48913  906663.2   38543.0\n",
      "49579  904291  914174  887490  890455   81385  905378.4   -2965.0\n",
      "49580  890455  894408  871677  885514   69075  903401.8  -61274.0\n",
      "49581  874642  886501  874642  875631   57862  903204.1  -68192.0\n",
      "49582  880572  882548  872665  880572   91909  903302.9  -43485.0\n",
      "49583  888478  894408  873654  879584   72407  901919.3  -38543.0]]\n",
      "[[         Open    High     Low   Close  Volume      ma10  momentum\n",
      "49584  947776  947776  885513  892432  175056  896483.7    1977.0]]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[873504.6, 888424.9, 847943.6, 869745.1, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/momentum']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', 'momentum']\n",
      "Epoch 1/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 1: val_loss improved from inf to 0.00125, saving model to ./checkpoint/LSTM\\LSTM_ma10_high_low_diff.h5\n",
      "992/992 [==============================] - 22s 16ms/step - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 2/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 2: val_loss did not improve from 0.00125\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0013\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 3: val_loss improved from 0.00125 to 0.00112, saving model to ./checkpoint/LSTM\\LSTM_ma10_high_low_diff.h5\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0011\n",
      "Epoch 4/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 4: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 5/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 5: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 6/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 6: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 7/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 7: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 8/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 8: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 9/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 9: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 10/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 10: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 11/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 11: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 12/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 12: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 13/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 13: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 14/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 14: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 15/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 15: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 13s 13ms/step - loss: 0.0058 - val_loss: 0.0013\n",
      "Epoch 16/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 16: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 13s 13ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 17/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 17: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 18/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 18: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 19/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 19: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 20/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 20: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 21/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 21: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 22/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 22: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 23/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 23: val_loss did not improve from 0.00112\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0058 - val_loss: 0.0012\n",
      "[[         Open    High     Low   Close  Volume      ma10  high_low_diff\n",
      "49564  879583  904291  875630  880572   96530  859225.0          28661\n",
      "49565  875630  885513  854876  881560   60446  864265.3          30637\n",
      "49566  865747  893420  861794  889467   59299  865056.0          31626\n",
      "49567  890455  898361  874642  890455   47530  867526.7          23719\n",
      "49568  875630  881560  839063  866736   94814  869700.9          42497\n",
      "49569  872665  909232  871677  903303  140974  876124.8          37555\n",
      "49570  903302  908244  889466  905280   97750  882944.1          18778\n",
      "49571  896384  905279  890455  890455   39993  886007.8          14824\n",
      "49572  896384  899349  860806  861795   79128  884821.9          38543\n",
      "49573  875630  884525  861794  879584   75796  884920.7          22731\n",
      "49574  888478  901326  884525  893420   80391  886205.5          16801\n",
      "49575  906267  947776  906267  946788  288779  892728.3          41509\n",
      "49576  941846  955682  931963  943823  141827  898163.9          23719\n",
      "49577  949752  950741  919115  924057   65625  901524.1          31626\n",
      "49578  925045  933940  913185  918127   48913  906663.2          20755\n",
      "49579  904291  914174  887490  890455   81385  905378.4          26684\n",
      "49580  890455  894408  871677  885514   69075  903401.8          22731\n",
      "49581  874642  886501  874642  875631   57862  903204.1          11859\n",
      "49582  880572  882548  872665  880572   91909  903302.9           9883\n",
      "49583  888478  894408  873654  879584   72407  901919.3          20754]]\n",
      "[[         Open    High     Low   Close  Volume      ma10  high_low_diff\n",
      "49584  947776  947776  885513  892432  175056  896483.7          62263]]\n",
      "1/1 [==============================] - 1s 926ms/step\n",
      "[874479.0, 888525.44, 848200.3, 869015.1, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma10/high_low_diff']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma10', 'high_low_diff']\n",
      "Epoch 1/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0088\n",
      "Epoch 1: val_loss improved from inf to 0.00438, saving model to ./checkpoint/LSTM\\LSTM_ma20_S&P500.h5\n",
      "992/992 [==============================] - 22s 16ms/step - loss: 0.0088 - val_loss: 0.0044\n",
      "Epoch 2/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0087\n",
      "Epoch 2: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0044\n",
      "Epoch 3/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 3: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0044\n",
      "Epoch 4/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 4: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 5/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 5: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 6/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 6: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 7/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 7: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0046\n",
      "Epoch 8/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 8: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 9/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 9: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 10/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 10: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 11/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 11: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 12/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 12: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 13/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 13: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 14/50\n",
      "987/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 14: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 13s 13ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 15/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 15: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0046\n",
      "Epoch 16/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 16: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 17/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 17: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0046\n",
      "Epoch 18/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 18: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 19/50\n",
      "987/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 19: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 12s 12ms/step - loss: 0.0087 - val_loss: 0.0046\n",
      "Epoch 20/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Epoch 20: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "Epoch 21/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0087\n",
      "Epoch 21: val_loss did not improve from 0.00438\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0087 - val_loss: 0.0045\n",
      "[[         Open    High     Low   Close  Volume       ma20       S&P500\n",
      "49564  879583  904291  875630  880572   96530  846475.95  4513.040039\n",
      "49565  875630  885513  854876  881560   60446  847810.15  4577.100098\n",
      "49566  865747  893420  861794  889467   59299  849589.10  4538.430176\n",
      "49567  890455  898361  874642  890455   47530  853443.45  4591.669922\n",
      "49568  875630  881560  839063  866736   94814  855963.60  4686.750000\n",
      "49569  872665  909232  871677  903303  140974  860608.60  4701.209961\n",
      "49570  903302  908244  889466  905280   97750  865401.85  4667.450195\n",
      "49571  896384  905279  890455  890455   39993  868762.05  4712.020020\n",
      "49572  896384  899349  860806  861795   79128  869157.40  4668.970215\n",
      "49573  875630  884525  861794  879584   75796  870046.85  4634.089844\n",
      "49574  888478  901326  884525  893420   80391  872715.25  4709.850098\n",
      "49575  906267  947776  906267  946788  288779  878496.80  4668.669922\n",
      "49576  941846  955682  931963  943823  141827  881609.95  4620.640137\n",
      "49577  949752  950741  919115  924057   65625  884525.40  4568.020020\n",
      "49578  925045  933940  913185  918127   48913  888182.05  4649.229980\n",
      "49579  904291  914174  887490  890455   81385  890751.60  4696.560059\n",
      "49580  890455  894408  871677  885514   69075  893172.95  4725.790039\n",
      "49581  874642  886501  874642  875631   57862  894013.00  4791.189941\n",
      "49582  880572  882548  872665  880572   91909  894111.80  4786.350098\n",
      "49583  888478  894408  873654  879584   72407  894062.40  4793.060059]]\n",
      "[[         Open    High     Low   Close  Volume      ma20      S&P500\n",
      "49584  947776  947776  885513  892432  175056  894606.0  4778.72998]]\n",
      "1/1 [==============================] - 1s 602ms/step\n",
      "[875404.2, 888790.94, 848298.6, 868454.94, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma20/S&P500']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma20', 'S&P500']\n",
      "Epoch 1/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0098\n",
      "Epoch 1: val_loss improved from inf to 0.00435, saving model to ./checkpoint/LSTM\\LSTM_ma20_Kospi.h5\n",
      "992/992 [==============================] - 21s 16ms/step - loss: 0.0098 - val_loss: 0.0043\n",
      "Epoch 2/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 2: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 3/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 3: val_loss did not improve from 0.00435\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 4/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 4: val_loss improved from 0.00435 to 0.00429, saving model to ./checkpoint/LSTM\\LSTM_ma20_Kospi.h5\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0043\n",
      "Epoch 5/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 5: val_loss did not improve from 0.00429\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 6/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 6: val_loss did not improve from 0.00429\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0043\n",
      "Epoch 7/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 7: val_loss improved from 0.00429 to 0.00424, saving model to ./checkpoint/LSTM\\LSTM_ma20_Kospi.h5\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0042\n",
      "Epoch 8/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 8: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 9/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 9: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0096 - val_loss: 0.0047\n",
      "Epoch 10/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 10: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 11/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 11: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 12/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 12: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 13/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0096\n",
      "Epoch 13: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 14/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 14: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 15/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 15: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 16/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 16: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 17/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 17: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 18/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 18: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 19/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 19: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 20/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 20: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 13s 13ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 21/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 21: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 22/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 22: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 23/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 23: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 24/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0096\n",
      "Epoch 24: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 25/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 25: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 26/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 26: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 27/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 27: val_loss did not improve from 0.00424\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "[[         Open    High     Low   Close  Volume       ma20        Kospi\n",
      "49564  879583  904291  875630  880572   96530  846475.95  2899.719971\n",
      "49565  875630  885513  854876  881560   60446  847810.15  2945.270020\n",
      "49566  865747  893420  861794  889467   59299  849589.10  2968.330078\n",
      "49567  890455  898361  874642  890455   47530  853443.45  2973.250000\n",
      "49568  875630  881560  839063  866736   94814  855963.60  2991.719971\n",
      "49569  872665  909232  871677  903303  140974  860608.60  3001.800049\n",
      "49570  903302  908244  889466  905280   97750  865401.85  3029.570068\n",
      "49571  896384  905279  890455  890455   39993  868762.05  3010.229980\n",
      "49572  896384  899349  860806  861795   79128  869157.40  3001.659912\n",
      "49573  875630  884525  861794  879584   75796  870046.85  2987.949951\n",
      "49574  888478  901326  884525  893420   80391  872715.25  2989.389893\n",
      "49575  906267  947776  906267  946788  288779  878496.80  3006.409912\n",
      "49576  941846  955682  931963  943823  141827  881609.95  3017.729980\n",
      "49577  949752  950741  919115  924057   65625  884525.40  2963.000000\n",
      "49578  925045  933940  913185  918127   48913  888182.05  2975.030029\n",
      "49579  904291  914174  887490  890455   81385  890751.60  2984.479980\n",
      "49580  890455  894408  871677  885514   69075  893172.95  2998.169922\n",
      "49581  874642  886501  874642  875631   57862  894013.00  2999.550049\n",
      "49582  880572  882548  872665  880572   91909  894111.80  3020.239990\n",
      "49583  888478  894408  873654  879584   72407  894062.40  2993.290039]]\n",
      "[[         Open    High     Low   Close  Volume      ma20        Kospi\n",
      "49584  947776  947776  885513  892432  175056  894606.0  2977.649902]]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[872152.94, 888946.75, 847652.94, 868394.6, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma20/Kospi']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma20', 'Kospi']\n",
      "Epoch 1/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0082\n",
      "Epoch 1: val_loss improved from inf to 0.00295, saving model to ./checkpoint/LSTM\\LSTM_ma20_rsi.h5\n",
      "992/992 [==============================] - 23s 16ms/step - loss: 0.0082 - val_loss: 0.0030\n",
      "Epoch 2/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 2: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 3/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 3: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 4/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0079\n",
      "Epoch 4: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0079 - val_loss: 0.0031\n",
      "Epoch 5/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0079\n",
      "Epoch 5: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0079 - val_loss: 0.0031\n",
      "Epoch 6/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0079\n",
      "Epoch 6: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 7/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0079\n",
      "Epoch 7: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 8/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 8: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 9/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 9: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 13s 14ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 10/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 10: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 16ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 11/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 11: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 12/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0079\n",
      "Epoch 12: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 13/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 13: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 14/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 14: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 15/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 15: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 16/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Epoch 16: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 17/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 17: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 18/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 18: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 19/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 19: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 20/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 20: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 13s 13ms/step - loss: 0.0078 - val_loss: 0.0031\n",
      "Epoch 21/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Epoch 21: val_loss did not improve from 0.00295\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0078 - val_loss: 0.0032\n",
      "[[         Open    High     Low   Close  Volume       ma20        rsi\n",
      "49564  879583  904291  875630  880572   96530  846475.95  66.063375\n",
      "49565  875630  885513  854876  881560   60446  847810.15  66.515729\n",
      "49566  865747  893420  861794  889467   59299  849589.10  65.581408\n",
      "49567  890455  898361  874642  890455   47530  853443.45  60.000000\n",
      "49568  875630  881560  839063  866736   94814  855963.60  51.243663\n",
      "49569  872665  909232  871677  903303  140974  860608.60  64.814893\n",
      "49570  903302  908244  889466  905280   97750  865401.85  67.942852\n",
      "49571  896384  905279  890455  890455   39993  868762.05  52.601257\n",
      "49572  896384  899349  860806  861795   79128  869157.40  48.924781\n",
      "49573  875630  884525  861794  879584   75796  870046.85  59.562751\n",
      "49574  888478  901326  884525  893420   80391  872715.25  64.397872\n",
      "49575  906267  947776  906267  946788  288779  878496.80  69.642873\n",
      "49576  941846  955682  931963  943823  141827  881609.95  66.666667\n",
      "49577  949752  950741  919115  924057   65625  884525.40  60.087604\n",
      "49578  925045  933940  913185  918127   48913  888182.05  58.189589\n",
      "49579  904291  914174  887490  890455   81385  890751.60  51.737515\n",
      "49580  890455  894408  871677  885514   69075  893172.95  49.218784\n",
      "49581  874642  886501  874642  875631   57862  894013.00  47.169891\n",
      "49582  880572  882548  872665  880572   91909  894111.80  52.845508\n",
      "49583  888478  894408  873654  879584   72407  894062.40  44.285707]]\n",
      "[[         Open    High     Low   Close  Volume      ma20       rsi\n",
      "49584  947776  947776  885513  892432  175056  894606.0  47.05877]]\n",
      "1/1 [==============================] - 1s 907ms/step\n",
      "[871782.2, 885393.7, 850258.7, 867582.06, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma20/rsi']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma20', 'rsi']\n",
      "Epoch 1/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 1: val_loss improved from inf to 0.00606, saving model to ./checkpoint/LSTM\\LSTM_ma20_%K.h5\n",
      "992/992 [==============================] - 21s 16ms/step - loss: 0.0112 - val_loss: 0.0061\n",
      "Epoch 2/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 2: val_loss improved from 0.00606 to 0.00601, saving model to ./checkpoint/LSTM\\LSTM_ma20_%K.h5\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 3: val_loss did not improve from 0.00601\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0109 - val_loss: 0.0061\n",
      "Epoch 4/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 4: val_loss improved from 0.00601 to 0.00592, saving model to ./checkpoint/LSTM\\LSTM_ma20_%K.h5\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 5/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 5: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0061\n",
      "Epoch 6/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 6: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 7/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 7: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 8/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 8: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 9/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 9: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 10/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 10: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 11/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 11: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 12/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 12: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 13/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 13: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0061\n",
      "Epoch 14/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 14: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 15/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 15: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 16/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 16: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 17/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 17: val_loss improved from 0.00592 to 0.00592, saving model to ./checkpoint/LSTM\\LSTM_ma20_%K.h5\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 18/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 18: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0061\n",
      "Epoch 19/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 19: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 20/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 20: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 21/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 21: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 22/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 22: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 23/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 23: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 24/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 24: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 25/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 25: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 26/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 26: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 27/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 27: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 28/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 28: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 29/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 29: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 30/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 30: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 31/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 31: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 32/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 32: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 33/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 33: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 34/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 34: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 35/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 35: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 36/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 36: val_loss improved from 0.00592 to 0.00592, saving model to ./checkpoint/LSTM\\LSTM_ma20_%K.h5\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 37/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 37: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 38/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 38: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 39/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 39: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 40/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 40: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 41/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 41: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 42/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 42: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0109 - val_loss: 0.0061\n",
      "Epoch 43/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 43: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 44/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 44: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 45/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 45: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 16s 17ms/step - loss: 0.0109 - val_loss: 0.0059\n",
      "Epoch 46/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 46: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 47/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 47: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 48/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 48: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 49/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 49: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "Epoch 50/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 50: val_loss did not improve from 0.00592\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0109 - val_loss: 0.0060\n",
      "[[         Open    High     Low   Close  Volume       ma20         %K\n",
      "49564  879583  904291  875630  880572   96530  846475.95  73.394972\n",
      "49565  875630  885513  854876  881560   60446  847810.15  72.000121\n",
      "49566  865747  893420  861794  889467   59299  849589.10  76.745144\n",
      "49567  890455  898361  874642  890455   47530  853443.45  77.907592\n",
      "49568  875630  881560  839063  866736   94814  855963.60  50.000588\n",
      "49569  872665  909232  871677  903303  140974  860608.60  93.024131\n",
      "49570  903302  908244  889466  905280   97750  865401.85  94.873325\n",
      "49571  896384  905279  890455  890455   39993  868762.05  75.641807\n",
      "49572  896384  899349  860806  861795   79128  869157.40  38.463035\n",
      "49573  875630  884525  861794  879584   75796  870046.85  61.539559\n",
      "49574  888478  901326  884525  893420   80391  872715.25  79.488111\n",
      "49575  906267  947776  906267  946788  288779  878496.80  99.091185\n",
      "49576  941846  955682  931963  943823  141827  881609.95  89.830988\n",
      "49577  949752  950741  919115  924057   65625  884525.40  72.881777\n",
      "49578  925045  933940  913185  918127   48913  888182.05  67.796843\n",
      "49579  904291  914174  887490  890455   81385  890751.60  44.068291\n",
      "49580  890455  894408  871677  885514   69075  893172.95  39.831417\n",
      "49581  874642  886501  874642  875631   57862  894013.00  31.356811\n",
      "49582  880572  882548  872665  880572   91909  894111.80  20.833509\n",
      "49583  888478  894408  873654  879584   72407  894062.40  19.792150]]\n",
      "[[         Open    High     Low   Close  Volume      ma20         %K\n",
      "49584  947776  947776  885513  892432  175056  894606.0  33.334036]]\n",
      "1/1 [==============================] - 1s 719ms/step\n",
      "[873298.56, 888319.5, 847321.7, 869199.1, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma20/%K']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma20', '%K']\n",
      "Epoch 1/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0114\n",
      "Epoch 1: val_loss improved from inf to 0.00616, saving model to ./checkpoint/LSTM\\LSTM_ma20_%D.h5\n",
      "992/992 [==============================] - 22s 18ms/step - loss: 0.0114 - val_loss: 0.0062\n",
      "Epoch 2/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 2: val_loss improved from 0.00616 to 0.00610, saving model to ./checkpoint/LSTM\\LSTM_ma20_%D.h5\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0112 - val_loss: 0.0061\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0112\n",
      "Epoch 3: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 4/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 4: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0061\n",
      "Epoch 5/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 5: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 6/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 6: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0061\n",
      "Epoch 7/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 7: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 8/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0112\n",
      "Epoch 8: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0063\n",
      "Epoch 9/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0112\n",
      "Epoch 9: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 10/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 10: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0063\n",
      "Epoch 11/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 11: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0063\n",
      "Epoch 12/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 12: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0112 - val_loss: 0.0061\n",
      "Epoch 13/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 13: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 16s 17ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 14/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 14: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0061\n",
      "Epoch 15/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 15: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0063\n",
      "Epoch 16/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 16: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 17/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0112\n",
      "Epoch 17: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 18/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0112\n",
      "Epoch 18: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0063\n",
      "Epoch 19/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 19: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 20/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 20: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0061\n",
      "Epoch 21/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0112\n",
      "Epoch 21: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "Epoch 22/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Epoch 22: val_loss did not improve from 0.00610\n",
      "992/992 [==============================] - 16s 17ms/step - loss: 0.0112 - val_loss: 0.0062\n",
      "[[         Open    High     Low   Close  Volume       ma20         %D\n",
      "49564  879583  904291  875630  880572   96530  846475.95  70.642877\n",
      "49565  875630  885513  854876  881560   60446  847810.15  72.318583\n",
      "49566  865747  893420  861794  889467   59299  849589.10  74.046746\n",
      "49567  890455  898361  874642  890455   47530  853443.45  75.550953\n",
      "49568  875630  881560  839063  866736   94814  855963.60  68.217775\n",
      "49569  872665  909232  871677  903303  140974  860608.60  73.644104\n",
      "49570  903302  908244  889466  905280   97750  865401.85  79.299348\n",
      "49571  896384  905279  890455  890455   39993  868762.05  87.846421\n",
      "49572  896384  899349  860806  861795   79128  869157.40  69.659389\n",
      "49573  875630  884525  861794  879584   75796  870046.85  58.548134\n",
      "49574  888478  901326  884525  893420   80391  872715.25  59.830235\n",
      "49575  906267  947776  906267  946788  288779  878496.80  80.039618\n",
      "49576  941846  955682  931963  943823  141827  881609.95  89.470095\n",
      "49577  949752  950741  919115  924057   65625  884525.40  87.267984\n",
      "49578  925045  933940  913185  918127   48913  888182.05  76.836536\n",
      "49579  904291  914174  887490  890455   81385  890751.60  61.582304\n",
      "49580  890455  894408  871677  885514   69075  893172.95  50.565517\n",
      "49581  874642  886501  874642  875631   57862  894013.00  38.418840\n",
      "49582  880572  882548  872665  880572   91909  894111.80  30.673912\n",
      "49583  888478  894408  873654  879584   72407  894062.40  23.994157]]\n",
      "[[         Open    High     Low   Close  Volume      ma20         %D\n",
      "49584  947776  947776  885513  892432  175056  894606.0  24.653232]]\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[872966.4, 888365.0, 848216.25, 872357.3, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma20/%D']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma20', '%D']\n",
      "Epoch 1/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 1: val_loss improved from inf to 0.00150, saving model to ./checkpoint/LSTM\\LSTM_ma20_bb_upper.h5\n",
      "992/992 [==============================] - 26s 18ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 2/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 2: val_loss improved from 0.00150 to 0.00138, saving model to ./checkpoint/LSTM\\LSTM_ma20_bb_upper.h5\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 3/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 3: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 4/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 4: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 5/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 5: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 6/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 6: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 7/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 7: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 8/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 8: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 9/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 9: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 10/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 10: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 19s 19ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 11/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0071\n",
      "Epoch 11: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 12/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 12: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 13/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 13: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 14/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 14: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 15/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 15: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 16/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 16: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 17/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 17: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 18s 18ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 18/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 18: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 16s 17ms/step - loss: 0.0071 - val_loss: 0.0014\n",
      "Epoch 19/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 19: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 20/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 20: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 21/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 21: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 16s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "Epoch 22/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Epoch 22: val_loss did not improve from 0.00138\n",
      "992/992 [==============================] - 16s 17ms/step - loss: 0.0071 - val_loss: 0.0015\n",
      "[[         Open    High     Low   Close  Volume       ma20       bb_upper\n",
      "49564  879583  904291  875630  880572   96530  846475.95  895809.711065\n",
      "49565  875630  885513  854876  881560   60446  847810.15  898567.385325\n",
      "49566  865747  893420  861794  889467   59299  849589.10  903495.716310\n",
      "49567  890455  898361  874642  890455   47530  853443.45  908264.144220\n",
      "49568  875630  881560  839063  866736   94814  855963.60  908147.109693\n",
      "49569  872665  909232  871677  903303  140974  860608.60  913422.548328\n",
      "49570  903302  908244  889466  905280   97750  865401.85  916744.202028\n",
      "49571  896384  905279  890455  890455   39993  868762.05  915296.773194\n",
      "49572  896384  899349  860806  861795   79128  869157.40  912425.017843\n",
      "49573  875630  884525  861794  879584   75796  870046.85  913209.106806\n",
      "49574  888478  901326  884525  893420   80391  872715.25  915660.219769\n",
      "49575  906267  947776  906267  946788  288779  878496.80  929537.195099\n",
      "49576  941846  955682  931963  943823  141827  881609.95  938078.852220\n",
      "49577  949752  950741  919115  924057   65625  884525.40  943081.683350\n",
      "49578  925045  933940  913185  918127   48913  888182.05  946595.053090\n",
      "49579  904291  914174  887490  890455   81385  890751.60  945119.576307\n",
      "49580  890455  894408  871677  885514   69075  893172.95  941792.129674\n",
      "49581  874642  886501  874642  875631   57862  894013.00  940779.773037\n",
      "49582  880572  882548  872665  880572   91909  894111.80  940593.051556\n",
      "49583  888478  894408  873654  879584   72407  894062.40  940573.892778]]\n",
      "[[         Open    High     Low   Close  Volume      ma20       bb_upper\n",
      "49584  947776  947776  885513  892432  175056  894606.0  940724.272154]]\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[874096.1, 888811.3, 846538.7, 872900.75, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma20/bb_upper']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma20', 'bb_upper']\n",
      "Epoch 1/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0073\n",
      "Epoch 1: val_loss improved from inf to 0.00153, saving model to ./checkpoint/LSTM\\LSTM_ma20_bb_lower.h5\n",
      "992/992 [==============================] - 26s 18ms/step - loss: 0.0073 - val_loss: 0.0015\n",
      "Epoch 2/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 2: val_loss improved from 0.00153 to 0.00144, saving model to ./checkpoint/LSTM\\LSTM_ma20_bb_lower.h5\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0072 - val_loss: 0.0014\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 3: val_loss did not improve from 0.00144\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 4/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 4: val_loss improved from 0.00144 to 0.00140, saving model to ./checkpoint/LSTM\\LSTM_ma20_bb_lower.h5\n",
      "992/992 [==============================] - 17s 17ms/step - loss: 0.0072 - val_loss: 0.0014\n",
      "Epoch 5/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 5: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 6/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 6: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0072 - val_loss: 0.0016\n",
      "Epoch 7/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 7: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0072 - val_loss: 0.0017\n",
      "Epoch 8/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 8: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 9/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 9: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 13s 13ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 10/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 10: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 11/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 11: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0014\n",
      "Epoch 12/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 12: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 13/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 13: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 14/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 14: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 15/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 15: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 16/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 16: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 17/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 17: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 18/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 18: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 19/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 19: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 20/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Epoch 20: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 21/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 21: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 15ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 22/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 22: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 23/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 23: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 24/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 24: val_loss did not improve from 0.00140\n",
      "992/992 [==============================] - 14s 14ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "[[         Open    High     Low   Close  Volume       ma20       bb_lower\n",
      "49564  879583  904291  875630  880572   96530  846475.95  799810.588935\n",
      "49565  875630  885513  854876  881560   60446  847810.15  798831.814675\n",
      "49566  865747  893420  861794  889467   59299  849589.10  797362.583690\n",
      "49567  890455  898361  874642  890455   47530  853443.45  796250.855780\n",
      "49568  875630  881560  839063  866736   94814  855963.60  801704.690307\n",
      "49569  872665  909232  871677  903303  140974  860608.60  805126.251672\n",
      "49570  903302  908244  889466  905280   97750  865401.85  811292.297972\n",
      "49571  896384  905279  890455  890455   39993  868762.05  820843.726806\n",
      "49572  896384  899349  860806  861795   79128  869157.40  827569.882157\n",
      "49573  875630  884525  861794  879584   75796  870046.85  829355.393194\n",
      "49574  888478  901326  884525  893420   80391  872715.25  830066.780231\n",
      "49575  906267  947776  906267  946788  288779  878496.80  826863.404901\n",
      "49576  941846  955682  931963  943823  141827  881609.95  829588.347780\n",
      "49577  949752  950741  919115  924057   65625  884525.40  828835.216650\n",
      "49578  925045  933940  913185  918127   48913  888182.05  830559.746910\n",
      "49579  904291  914174  887490  890455   81385  890751.60  836581.323693\n",
      "49580  890455  894408  871677  885514   69075  893172.95  844553.770326\n",
      "49581  874642  886501  874642  875631   57862  894013.00  847147.426963\n",
      "49582  880572  882548  872665  880572   91909  894111.80  848025.948444\n",
      "49583  888478  894408  873654  879584   72407  894062.40  848143.907222]]\n",
      "[[         Open    High     Low   Close  Volume      ma20       bb_lower\n",
      "49584  947776  947776  885513  892432  175056  894606.0  849179.527846]]\n",
      "1/1 [==============================] - 1s 818ms/step\n",
      "[874311.3, 888689.1, 847586.8, 871142.1, 947776.0, 947776.0, 885513.0, 892432.0, 'Open/High/Low/Close/Volume/ma20/bb_lower']\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'ma20', 'bb_lower']\n",
      "Epoch 1/50\n",
      "988/992 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Epoch 1: val_loss improved from inf to 0.00128, saving model to ./checkpoint/LSTM\\LSTM_ma20_volume_ma5.h5\n",
      "992/992 [==============================] - 21s 16ms/step - loss: 0.0060 - val_loss: 0.0013\n",
      "Epoch 2/50\n",
      "989/992 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Epoch 2: val_loss improved from 0.00128 to 0.00118, saving model to ./checkpoint/LSTM\\LSTM_ma20_volume_ma5.h5\n",
      "992/992 [==============================] - 15s 15ms/step - loss: 0.0060 - val_loss: 0.0012\n",
      "Epoch 3/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0060\n",
      "Epoch 3: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 16s 17ms/step - loss: 0.0060 - val_loss: 0.0013\n",
      "Epoch 4/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Epoch 4: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0060 - val_loss: 0.0013\n",
      "Epoch 5/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0060\n",
      "Epoch 5: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0060 - val_loss: 0.0012\n",
      "Epoch 6/50\n",
      "992/992 [==============================] - ETA: 0s - loss: 0.0060\n",
      "Epoch 6: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0060 - val_loss: 0.0013\n",
      "Epoch 7/50\n",
      "990/992 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Epoch 7: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 16s 16ms/step - loss: 0.0060 - val_loss: 0.0012\n",
      "Epoch 8/50\n",
      "991/992 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Epoch 8: val_loss did not improve from 0.00118\n",
      "992/992 [==============================] - 15s 16ms/step - loss: 0.0060 - val_loss: 0.0012\n",
      "Epoch 9/50\n",
      "665/992 [===================>..........] - ETA: 4s - loss: 0.0059"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# 스케일을 적용할 column을 정의합니다.\n",
    "# 'Close', 'Open', 'High', 'Low', ' Close', 'ma10', 'ma20', 'rsi', '%K', '%D', 'bb_upper', 'bb_sma', 'bb_lower', 'volume_ma5', 'momentum', 'high_low_diff'\n",
    "use_cols = ['Open', 'High', 'Low','Close','Volume']\n",
    "\n",
    "colList = ['ma10', 'ma20', 'S&P500','Kospi', 'rsi', '%K', '%D', 'bb_upper', 'bb_lower', 'volume_ma5', 'momentum', 'high_low_diff']\n",
    "# Date,Open,High,Low,Close,Volume,Change,ma5,ma10,ma20,rsi,%K,%D,bb_upper,bb_sma,bb_lower,volume_ma5,momentum,high_low_diff\n",
    "result_pridict = []\n",
    "com_colList = []\n",
    "\n",
    "for r in range(2,3):\n",
    "    # colList의 요소들 중 r개를 선택하는 모든 조합 생성\n",
    "    for combination in combinations(colList, r):\n",
    "        com_colList.append(list(combination))\n",
    "print(com_colList)\n",
    "\n",
    "for plus_cols in com_colList:\n",
    "    use_cols.extend(plus_cols)\n",
    "    \n",
    "    # 스케일 후 columns\n",
    "    scaled = scaler.fit_transform(stock[use_cols])\n",
    "    scaled\n",
    "    df = pd.DataFrame(scaled, columns=use_cols)\n",
    "\n",
    "    ln_use_cols = len(use_cols)\n",
    "    WINDOW_SIZE = 20    \n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(WINDOW_SIZE, len(scaled)):\n",
    "        X.append(scaled[i - WINDOW_SIZE:i])\n",
    "        y.append([scaled[i]])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # 데이터를 훈련 세트와 테스트 세트로 나눕니다.\n",
    "    split_ratio = 0.8  # 예시로 80%를 훈련 데이터로 사용\n",
    "    split_index = int(split_ratio * len(X))\n",
    "\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "\n",
    "    # LSTM 모델 설정\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='tanh', input_shape=(WINDOW_SIZE, ln_use_cols)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(ln_use_cols)  # 예측할 feature 개수\n",
    "    ])\n",
    "\n",
    "    # 모델 컴파일\n",
    "    loss = Huber()\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    # Early Stopping 설정\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    tmp = '_'.join(use_cols[5:])\n",
    "    # Model Checkpoint 설정\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=f'./checkpoint/LSTM/LSTM_{tmp}.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "        X_train,  # 훈련 데이터\n",
    "        y_train,  # 레이블\n",
    "        validation_split=0.2,  # 검증 데이터 비율\n",
    "        epochs=50,  # 에포크 횟수\n",
    "        callbacks=[early_stopping, checkpoint],  # Early Stopping과 Model Checkpoint 콜백\n",
    "        batch_size=32  # 배치 크기\n",
    "    )\n",
    "    input_scaled = stock[use_cols]\n",
    "\n",
    "    total_data =[[input_scaled[len(input_scaled)-WINDOW_SIZE-1:len(input_scaled)-1]]]\n",
    "    conform_total_data =[[input_scaled[len(input_scaled)-1:len(input_scaled)]]][:4]\n",
    "    print(total_data)\n",
    "    print(conform_total_data)\n",
    "\n",
    "    # total_data를 2D 배열로 변환\n",
    "    data = np.array(total_data)\n",
    "    # 2D 배열을 리스트로 변환\n",
    "    data = data.tolist()\n",
    "\n",
    "    # 데이터만 포함된 행렬 생성\n",
    "    data = data[0]\n",
    "\n",
    "    # data_matrix 출력\n",
    "    # print(data)\n",
    "    # print(len(data[0]))\n",
    "    conform_data = conform_total_data[0][0] # 마지막 데이터를 변수에 할당\n",
    "    conform_data = conform_data.values[0]  # 데이터 확인용 \n",
    "\n",
    "    # 3D 배열을 2D로 변환\n",
    "    data_2d = np.array(data).reshape(-1, len(data[0][0]))  # 하위 리스트의 길이를 사용\n",
    "\n",
    "    # Min-Max 스케일러 생성\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data_2d)\n",
    "\n",
    "    # 주어진 데이터를 스케일링\n",
    "    scaled_data_2d = scaler.transform(data_2d)\n",
    "\n",
    "    # 2D 배열을 3D로 다시 변환\n",
    "    scaled_data_3d = scaled_data_2d.reshape(len(data), len(data[0]), len(data[0][0]))  # 원래 형태로 다시 변환\n",
    "    model = load_model(f'./checkpoint/LSTM/LSTM_{tmp}.h5')\n",
    "    # 모델로부터의 예측 결과 (여기에서 모델 예측을 수행해야 함)\n",
    "    predicted_value = model.predict(scaled_data_3d)  # 모델에 스케일링된 데이터를 입력\n",
    "\n",
    "    # 예측값을 스케일링 전의 원래 값으로 역 스케일링\n",
    "    predicted_value_original = scaler.inverse_transform(predicted_value)\n",
    "    # print(predicted_value_original)\n",
    "    predicted_value_original = np.round(predicted_value_original, ln_use_cols)[0][:4]\n",
    "    # print(\"예측결과\",predicted_value_original)\n",
    "    conform_total_data =conform_total_data[0][0].values[0]\n",
    "    # print(\"넣은 데이터\",conform_total_data)\n",
    "    predicted_value_original = list(predicted_value_original)\n",
    "    predicted_value_original.extend(conform_total_data[:4])\n",
    "    # print(conform_total_data, \"컨텀 토탈\")\n",
    "    use_cols_check = '/'.join(use_cols)\n",
    "    predicted_value_original.append(use_cols_check)\n",
    "    result_use_cols = use_cols[:4]\n",
    "    result_use_cols.extend(['_Open', '_High', '_Low','_Close', 'Use_Cols_Check'])\n",
    "    # print(result_use_cols)\n",
    "    print(predicted_value_original)\n",
    "    result_dict = dict(zip(result_use_cols, predicted_value_original))\n",
    "    result_pridict.append(result_dict)\n",
    "    print(use_cols)\n",
    "    use_cols = use_cols[:5]  # 초기 use_cols로 재설정\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Open': 872607.7, 'High': 885758.3, 'Low': 847465.7, 'Close': 868226.94, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/ma10'}, {'Open': 873255.94, 'High': 887038.4, 'Low': 849376.8, 'Close': 870182.9, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/ma20'}, {'Open': 872929.7, 'High': 889275.25, 'Low': 847878.9, 'Close': 869393.25, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/S&P500'}, {'Open': 873247.56, 'High': 888487.8, 'Low': 846898.3, 'Close': 869974.6, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/Kospi'}, {'Open': 873275.2, 'High': 886383.5, 'Low': 846795.5, 'Close': 869061.3, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/rsi'}, {'Open': 872792.5, 'High': 887701.8, 'Low': 846745.94, 'Close': 868922.06, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/%K'}, {'Open': 874062.4, 'High': 888107.9, 'Low': 847107.1, 'Close': 869126.6, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/%D'}, {'Open': 873054.94, 'High': 887728.94, 'Low': 847286.6, 'Close': 868753.8, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/bb_upper'}, {'Open': 873947.6, 'High': 887817.2, 'Low': 847519.25, 'Close': 869079.0, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/bb_lower'}, {'Open': 872775.6, 'High': 887834.1, 'Low': 847414.0, 'Close': 868935.56, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/volume_ma5'}, {'Open': 872109.1, 'High': 887693.4, 'Low': 846422.3, 'Close': 869180.5, '_Open': 947776.0, '_High': 947776.0, '_Low': 885513.0, '_Close': 892432.0, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/momentum'}, {'Open': 872232.75, 'High': 887810.9, 'Low': 847403.6, 'Close': 867945.8, '_Open': 947776, '_High': 947776, '_Low': 885513, '_Close': 892432, 'Use_Cols_Check': 'Open/High/Low/Close/Volume/high_low_diff'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(result_pridict)\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 데이터를 DataFrame으로 변환\n",
    "df = pd.DataFrame(result_pridict)\n",
    "\n",
    "# DataFrame을 엑셀 파일로 저장\n",
    "df.to_excel('LSTM_result_pridict2.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/333 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 64)                18432     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 7)                 231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20743 (81.03 KB)\n",
      "Trainable params: 20743 (81.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 사용하여 예측\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 예측값을 스케일 역변환 (원래 스케일로 되돌리기)\n",
    "y_pred = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 원래 데이터와 예측값 비교\n",
    "y_test_original = scaler.inverse_transform(y_test[:, -1])  # y_test를 1D로 변환\n",
    "y_pred_original = y_pred  # y_pred의 형태가 이미 맞음\n",
    "\n",
    "# 예측값과 실제값 비교 및 그래프 그리기\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.figure(figsize=(12, ln_use_cols))\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred_original, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Close Price')\n",
    "plt.show()\n",
    "\n",
    "# 모델의 요약 정보 출력\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.40000000e+05  1.40500000e+05  1.38000000e+05  1.38000000e+05\n",
      "  7.03821000e+05 -1.77935943e-02  1.30450000e+05]\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "[ 1.30582797e+05  1.32634281e+05  1.29567375e+05  1.29861312e+05\n",
      "  7.83532250e+05 -3.61429993e-03  1.26739492e+05]\n",
      "[6.73, 5.6, 6.11, 5.9, 11.33, 79.69, 2.84]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "input_use_col = ['Open', 'High', 'Low', ' Close','Volume', 'Change', 'ma20']\n",
    "input_scaled = stock[input_use_col]\n",
    "\n",
    "total_data =[[input_scaled[len(input_scaled)-WINDOW_SIZE:len(input_scaled)]]]\n",
    "conform_total_data =[[input_scaled[len(input_scaled)-1:len(input_scaled)]]]\n",
    "# print(total_data)\n",
    "# print(conform_total_data)\n",
    "\n",
    "# total_data를 2D 배열로 변환\n",
    "data = np.array(total_data)\n",
    "# 2D 배열을 리스트로 변환\n",
    "data = data.tolist()\n",
    "\n",
    "# 데이터만 포함된 행렬 생성\n",
    "data = data[0]\n",
    "\n",
    "# data_matrix 출력\n",
    "# print(data)\n",
    "# print(len(data[0]))\n",
    "conform_data = conform_total_data[0][0] # 마지막 데이터를 변수에 할당\n",
    "conform_data = conform_data.values[0]  # 데이터 확인용 \n",
    "print(conform_data)\n",
    "\n",
    "\n",
    "# 3D 배열을 2D로 변환\n",
    "data_2d = np.array(data).reshape(-1, len(data[0][0]))  # 하위 리스트의 길이를 사용\n",
    "\n",
    "# Min-Max 스케일러 생성\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_2d)\n",
    "\n",
    "# 주어진 데이터를 스케일링\n",
    "scaled_data_2d = scaler.transform(data_2d)\n",
    "\n",
    "# 2D 배열을 3D로 다시 변환\n",
    "scaled_data_3d = scaled_data_2d.reshape(len(data), len(data[0]), len(data[0][0]))  # 원래 형태로 다시 변환\n",
    "\n",
    "# 모델로부터의 예측 결과 (여기에서 모델 예측을 수행해야 함)\n",
    "predicted_value = model.predict(scaled_data_3d)  # 모델에 스케일링된 데이터를 입력\n",
    "\n",
    "# 예측값을 스케일링 전의 원래 값으로 역 스케일링\n",
    "predicted_value_original = scaler.inverse_transform(predicted_value)\n",
    "# print(predicted_value_original)\n",
    "predicted_value_original = np.round(predicted_value_original, ln_use_cols)[0]\n",
    "print(predicted_value_original)\n",
    "\n",
    "# conform_data를 기준으로 predicted_value_original의 값을 백분율로 계산 후 절대값으로 만들기\n",
    "percentage_diff = [abs(round((a - b) / b * 100, 2)) for a, b in zip(predicted_value_original, conform_data)]\n",
    "\n",
    "print(percentage_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 311ms/step\n",
      "Predicted Value: [[3.225136]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
